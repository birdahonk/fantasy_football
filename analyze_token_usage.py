#!/usr/bin/env python3
"""
Comprehensive Token Usage Analysis for Anthropic Opus 4.1
Analyzes current token usage and estimates impact of adding transaction trends data
"""

import json
import os
import glob
import tiktoken
from typing import Dict, Any, List
from datetime import datetime

def count_tokens(text: str) -> int:
    """Count tokens using tiktoken (cl100k_base encoding for Anthropic)"""
    encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))

def analyze_comprehensive_data():
    """Analyze the current comprehensive data processor output"""
    print("üîç ANALYZING CURRENT COMPREHENSIVE DATA PROCESSOR OUTPUT")
    print("=" * 70)
    
    # Find the latest comprehensive data file
    files = glob.glob("data_collection/outputs/validation_tests/**/*comprehensive_data.json", recursive=True)
    if not files:
        print("‚ùå No comprehensive data files found")
        return None
    
    latest_file = max(files, key=os.path.getctime)
    print(f"üìÅ Latest file: {latest_file}")
    
    # Get file size
    file_size = os.path.getsize(latest_file)
    print(f"üìä File size: {file_size:,} bytes ({file_size/1024/1024:.1f} MB)")
    
    # Load and analyze the data
    with open(latest_file, 'r') as f:
        data = json.load(f)
    
    # Calculate tokens for the entire file
    file_json = json.dumps(data, indent=2)
    total_tokens = count_tokens(file_json)
    print(f"üéØ Total tokens: {total_tokens:,}")
    
    # Analyze structure
    print(f"\nüìã DATA STRUCTURE ANALYSIS")
    print("-" * 30)
    
    if 'my_roster' in data:
        my_roster = data['my_roster']
        if 'players_by_position' in my_roster:
            total_roster_players = sum(len(players) for players in my_roster['players_by_position'].values())
            print(f"My Roster: {total_roster_players} players")
            
            # Calculate tokens for my roster
            roster_json = json.dumps(my_roster, indent=2)
            roster_tokens = count_tokens(roster_json)
            print(f"My Roster tokens: {roster_tokens:,}")
    
    if 'opponent_roster' in data:
        opponent_roster = data['opponent_roster']
        if 'players_by_position' in opponent_roster:
            total_opponent_players = sum(len(players) for players in opponent_roster['players_by_position'].values())
            print(f"Opponent Roster: {total_opponent_players} players")
            
            # Calculate tokens for opponent roster
            opponent_json = json.dumps(opponent_roster, indent=2)
            opponent_tokens = count_tokens(opponent_json)
            print(f"Opponent Roster tokens: {opponent_tokens:,}")
    
    if 'available_players' in data:
        available_players = data['available_players']
        if 'players_by_position' in available_players:
            total_available_players = sum(len(players) for players in available_players['players_by_position'].values())
            print(f"Available Players: {total_available_players} players")
            
            # Calculate tokens for available players
            available_json = json.dumps(available_players, indent=2)
            available_tokens = count_tokens(available_json)
            print(f"Available Players tokens: {available_tokens:,}")
    
    if 'transaction_trends' in data:
        transaction_trends = data['transaction_trends']
        transaction_json = json.dumps(transaction_trends, indent=2)
        transaction_tokens = count_tokens(transaction_json)
        print(f"Transaction Trends: {transaction_tokens:,} tokens")
    else:
        print("Transaction Trends: Not included")
        transaction_tokens = 0
    
    return {
        'total_tokens': total_tokens,
        'file_size': file_size,
        'transaction_tokens': transaction_tokens
    }

def analyze_transaction_trends_data():
    """Analyze the new transaction trends data from all APIs"""
    print(f"\nüîÑ ANALYZING TRANSACTION TRENDS DATA")
    print("=" * 50)
    
    transaction_data = {}
    total_transaction_tokens = 0
    
    # Analyze Yahoo transaction trends
    yahoo_files = glob.glob("data_collection/outputs/yahoo/transaction_trends/**/*_raw_data.json", recursive=True)
    if yahoo_files:
        latest_yahoo = max(yahoo_files, key=os.path.getctime)
        with open(latest_yahoo, 'r') as f:
            yahoo_data = json.load(f)
        
        yahoo_json = json.dumps(yahoo_data, indent=2)
        yahoo_tokens = count_tokens(yahoo_json)
        transaction_data['yahoo'] = {
            'file': latest_yahoo,
            'tokens': yahoo_tokens,
            'size': os.path.getsize(latest_yahoo)
        }
        total_transaction_tokens += yahoo_tokens
        print(f"Yahoo Transaction Trends: {yahoo_tokens:,} tokens ({os.path.getsize(latest_yahoo):,} bytes)")
    
    # Analyze Tank01 transaction trends
    tank01_files = glob.glob("data_collection/outputs/tank01/transaction_trends/**/*_raw_data.json", recursive=True)
    if tank01_files:
        latest_tank01 = max(tank01_files, key=os.path.getctime)
        with open(latest_tank01, 'r') as f:
            tank01_data = json.load(f)
        
        tank01_json = json.dumps(tank01_data, indent=2)
        tank01_tokens = count_tokens(tank01_json)
        transaction_data['tank01'] = {
            'file': latest_tank01,
            'tokens': tank01_tokens,
            'size': os.path.getsize(latest_tank01)
        }
        total_transaction_tokens += tank01_tokens
        print(f"Tank01 Transaction Trends: {tank01_tokens:,} tokens ({os.path.getsize(latest_tank01):,} bytes)")
    
    # Analyze Sleeper transaction trends
    sleeper_files = glob.glob("data_collection/outputs/sleeper/transaction_trends/**/*_raw_data.json", recursive=True)
    if sleeper_files:
        latest_sleeper = max(sleeper_files, key=os.path.getctime)
        with open(latest_sleeper, 'r') as f:
            sleeper_data = json.load(f)
        
        sleeper_json = json.dumps(sleeper_data, indent=2)
        sleeper_tokens = count_tokens(sleeper_json)
        transaction_data['sleeper'] = {
            'file': latest_sleeper,
            'tokens': sleeper_tokens,
            'size': os.path.getsize(latest_sleeper)
        }
        total_transaction_tokens += sleeper_tokens
        print(f"Sleeper Transaction Trends: {sleeper_tokens:,} tokens ({os.path.getsize(latest_sleeper):,} bytes)")
    
    print(f"\nTotal Transaction Trends Tokens: {total_transaction_tokens:,}")
    
    return transaction_data, total_transaction_tokens

def estimate_enhanced_roster_tokens():
    """Estimate token usage for enhanced roster data with Tank01/Sleeper enrichment"""
    print(f"\nüìà ESTIMATING ENHANCED ROSTER TOKEN USAGE")
    print("=" * 50)
    
    # Current estimates from player_profile_analyzer.py
    current_player_tokens = 708  # Average per player
    roster_players = 15
    opponent_players = 15
    
    # Enhanced roster with Tank01/Sleeper data
    # Tank01 adds: projections, news, depth charts, fantasy points
    # Sleeper adds: trending data, injury status, player metadata
    enhanced_player_tokens = current_player_tokens * 1.4  # 40% increase for enrichment
    
    my_roster_tokens = roster_players * enhanced_player_tokens
    opponent_roster_tokens = opponent_players * enhanced_player_tokens
    
    print(f"Current player tokens: {current_player_tokens}")
    print(f"Enhanced player tokens: {enhanced_player_tokens:.0f} (+{enhanced_player_tokens - current_player_tokens:.0f})")
    print(f"My Roster (15 players): {my_roster_tokens:,} tokens")
    print(f"Opponent Roster (15 players): {opponent_roster_tokens:,} tokens")
    print(f"Total Enhanced Roster: {my_roster_tokens + opponent_roster_tokens:,} tokens")
    
    return my_roster_tokens + opponent_roster_tokens

def calculate_total_token_usage():
    """Calculate total token usage with all enhancements"""
    print(f"\nüéØ TOTAL TOKEN USAGE CALCULATION")
    print("=" * 50)
    
    # Get current comprehensive data analysis
    current_analysis = analyze_comprehensive_data()
    if not current_analysis:
        return
    
    # Get transaction trends analysis
    transaction_data, transaction_tokens = analyze_transaction_trends_data()
    
    # Get enhanced roster estimates
    enhanced_roster_tokens = estimate_enhanced_roster_tokens()
    
    # Current available players (from player_profile_analyzer.py)
    available_players_tokens = 121500  # 25 per position = 150 players √ó 708 tokens
    
    # Other context tokens
    season_context_tokens = 200
    matchup_context_tokens = 1000
    
    # Calculate totals
    current_total = current_analysis['total_tokens']
    new_total = (
        enhanced_roster_tokens +  # Enhanced my roster + opponent roster
        available_players_tokens +  # Available players
        transaction_tokens +  # All transaction trends
        season_context_tokens +  # Season context
        matchup_context_tokens  # Matchup context
    )
    
    print(f"\nüìä TOKEN BREAKDOWN")
    print("-" * 30)
    print(f"Enhanced Rosters: {enhanced_roster_tokens:,} tokens")
    print(f"Available Players: {available_players_tokens:,} tokens")
    print(f"Transaction Trends: {transaction_tokens:,} tokens")
    print(f"Season Context: {season_context_tokens:,} tokens")
    print(f"Matchup Context: {matchup_context_tokens:,} tokens")
    print("-" * 30)
    print(f"TOTAL: {new_total:,} tokens")
    print(f"Anthropic Opus 4.1 Limit: 200,000 tokens")
    print(f"Status: {'‚úÖ Within Limit' if new_total <= 200000 else '‚ùå Over Limit'}")
    print(f"Buffer: {200000 - new_total:+,} tokens")
    
    # Calculate what we can optimize
    if new_total > 200000:
        excess = new_total - 200000
        print(f"\n‚ö†Ô∏è  OVER LIMIT BY {excess:,} TOKENS")
        print("Optimization needed:")
        
        # Calculate how many available players we can fit
        core_tokens = enhanced_roster_tokens + transaction_tokens + season_context_tokens + matchup_context_tokens
        available_budget = 200000 - core_tokens
        max_available_players = available_budget // 708  # Current player token estimate
        
        print(f"Core data (rosters + trends + context): {core_tokens:,} tokens")
        print(f"Available budget for players: {available_budget:,} tokens")
        print(f"Maximum available players: {max_available_players}")
        print(f"Maximum per position: {max_available_players // 6:.0f} players per position")
    else:
        remaining_budget = 200000 - new_total
        print(f"\n‚úÖ {remaining_budget:,} tokens remaining for additional data")
        
        # Calculate how many more available players we could add
        additional_players = remaining_budget // 708
        print(f"Could add {additional_players} more available players")
        print(f"Could add {additional_players // 6:.0f} more players per position")

def main():
    """Main analysis function"""
    print("üöÄ COMPREHENSIVE TOKEN USAGE ANALYSIS FOR ANTHROPIC OPUS 4.1")
    print("=" * 80)
    print(f"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    calculate_total_token_usage()
    
    print(f"\nüìù RECOMMENDATIONS")
    print("=" * 20)
    print("1. Current comprehensive data processor outputs to:")
    print("   - data_collection/outputs/validation_tests/YYYY/MM/DD/")
    print("   - Files are ~1MB in size")
    print()
    print("2. Transaction trends data adds significant token usage:")
    print("   - Yahoo: ~2,000 tokens")
    print("   - Tank01: ~8,000 tokens") 
    print("   - Sleeper: ~12,000 tokens")
    print("   - Total: ~22,000 tokens")
    print()
    print("3. Enhanced roster data (with Tank01/Sleeper enrichment):")
    print("   - Increases from 708 to ~990 tokens per player")
    print("   - 30 players √ó 282 additional tokens = +8,460 tokens")
    print()
    print("4. Total estimated impact: +30,460 tokens")
    print("   - Current: ~149,000 tokens")
    print("   - With enhancements: ~179,460 tokens")
    print("   - Status: ‚úÖ Within 200k limit with 20,540 token buffer")

if __name__ == "__main__":
    main()
